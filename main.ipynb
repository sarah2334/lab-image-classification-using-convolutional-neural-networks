{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Lab : Image Classification using Convolutional Neural Networks\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **In case you use a colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select ‚ÄúChange runtime type‚Äù as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure ‚ÄúHardware accelerator‚Äù is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra9DHcILtzuj",
        "outputId": "cf4ef60b-0643-430d-ed6e-9ac2383d9e72"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrb20KGMtTFq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fR_DuteFqcKm",
        "outputId": "c3eebad9-7bbe-41ac-fc0a-ad980681c856"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Normalize the images\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoded form\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Create a 10x10 plot showing 10 random samples from each class\n",
        "num_classes = 10\n",
        "samples_per_class = 10\n",
        "fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(15, 15))\n",
        "\n",
        "for class_index in range(num_classes):\n",
        "    # Get indices of images for the current class\n",
        "    class_indices = np.where(y_train.flatten() == class_index)[0]\n",
        "    # Randomly select 10 images from the current class\n",
        "    selected_indices = np.random.choice(class_indices, size=samples_per_class, replace=False)\n",
        "\n",
        "    for sample_index, img_index in enumerate(selected_indices):\n",
        "        # Plot the image\n",
        "        axes[class_index, sample_index].imshow(x_train[img_index])\n",
        "        axes[class_index, sample_index].axis('off')\n",
        "\n",
        "# Set titles for each row\n",
        "for ax, class_label in zip(axes[:, 0], range(num_classes)):\n",
        "    ax.set_ylabel(f'Class {class_label}', size=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "\n",
        "Use the input as (32,32,3).\n",
        "\n",
        "The filter maps can then be flattened to provide features to the classifier.\n",
        "\n",
        "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "iSN6riPISBMG",
        "outputId": "0bfe5fe4-52ca-4e77-a749-5c6658760341"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.backend import clear_session\n",
        "\n",
        "# Clear previous Keras session\n",
        "clear_session()\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "# Max pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the feature maps\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer with 100 units and ReLU activation\n",
        "model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# Output layer with softmax activation for classification\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn8UzPBZugVp",
        "outputId": "a5803e73-18e9-49a3-a229-68d1c64cf18b"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train_one_hot,\n",
        "                    batch_size=512,\n",
        "                    epochs=50,\n",
        "                    validation_data=(x_test, y_test_one_hot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11-jgekjqcKn"
      },
      "source": [
        "*   Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "IEnFuXkQqcKn",
        "outputId": "49bd6cee-5d45-4283-96f2-4fbccd10ca9e"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "\n",
        "Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "\n",
        "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgca5dUNSFNc",
        "outputId": "2f846440-9c2e-4d19-c05d-13ae2068b9c9"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.backend import clear_session\n",
        "\n",
        "# Clear previous Keras session\n",
        "clear_session()\n",
        "\n",
        "# Define the VGG-like model\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional block\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "# Max pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the feature maps\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer with 128 units\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Output layer with softmax activation for classification\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train_one_hot,\n",
        "                    batch_size=512,\n",
        "                    epochs=50,\n",
        "                    validation_data=(x_test, y_test_one_hot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc2qtU0mUvVA",
        "outputId": "a3d0cf2e-7791-4e5f-a519-8637152229f2"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train_one_hot,\n",
        "                    batch_size=512,\n",
        "                    epochs=50,\n",
        "                    validation_data=(x_test, y_test_one_hot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "F8OSHAf5SJPr",
        "outputId": "3e3c9a24-c055-4d2b-93e0-70214670829e"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'history1' is the training history of the first model\n",
        "# and 'history2' is the training history of the VGG-like deeper model\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Model 1 Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Model 1 Validation Loss')\n",
        "plt.plot(history.history['loss'], label='Model 2 Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Model 2 Validation Loss')\n",
        "plt.title('Loss Curve Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Model 1 Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Model 1 Validation Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='Model 2 Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Model 2 Validation Accuracy')\n",
        "plt.title('Accuracy Curve Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "**Comment on the observation**\n",
        "\n",
        "*(Loss Curves:\n",
        "\n",
        "Training Loss: Both models' training loss should ideally decrease over epochs. If the deeper model has a lower training loss, it suggests better fitting to the training data.\n",
        "\n",
        "Validation Loss: Compare the validation losses. If the deeper model shows a lower validation loss, it indicates better generalization to unseen data. If validation loss increases while training loss decreases, it suggests overfitting.\n",
        "\n",
        "Accuracy Curves:\n",
        "\n",
        "Training Accuracy: Higher training accuracy in the deeper model indicates it is capturing more patterns from the training data.\n",
        "\n",
        "Validation Accuracy: The model with higher validation accuracy is performing better on unseen data. If the deeper model consistently has a higher validation accuracy, it suggests that the additional layers help in learning more complex features.\n",
        "\n",
        "Overall Performance:\n",
        "\n",
        "If the deeper model performs better (lower loss and higher accuracy), it indicates that the architecture's depth is beneficial for this dataset.\n",
        "If both models perform similarly, the simpler model might be sufficient, indicating that increasing model complexity does not always yield better results.)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "*   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DObaoxhaSMUg",
        "outputId": "3a0926b4-06c7-4a33-94c3-013a8aea948b"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "import numpy as np\n",
        "\n",
        "# Predict the output for the test split\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "nwn3HXoCzLe7",
        "outputId": "dee8f80d-b012-4938-f17f-b2df7b3b92c2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_test contains the true labels in one-hot encoding\n",
        "true_classes = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                         'Dog', 'Frog', 'Horse', 'Ship', 'Truck'],\n",
        "            yticklabels=['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                         'Dog', 'Frog', 'Horse', 'Ship', 'Truck'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Comment here :**\n",
        "\n",
        "*(\n",
        "  1-Diagonal Elements: The values along the diagonal represent the correctly classified instances for each class. Higher values indicate better performance for those classes.\n",
        "\n",
        "2-Off-Diagonal Elements: These represent misclassifications. For example, if a significant number of \"Dog\" instances are misclassified as \"Cat,\" this indicates confusion between these two classes.\n",
        "\n",
        "3-Class Confusions:\n",
        "Identify which classes are most commonly confused with each other. For instance, if both \"Cat\" and \"Dog\" have high off-diagonal values, it may suggest similar features or characteristics that the model cannot distinguish well.\n",
        "Classes that are visually similar (e.g., \"Deer\" and \"Horse\") or have overlapping characteristics may also show higher confusion rates.)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        "*    Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WX3_uLSN5I",
        "outputId": "26621805-0e4f-41af-de35-f3b170dc3ef4"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test_one_hot)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "\n",
        "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "\n",
        "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "oH4lDVBuVA_Q",
        "outputId": "5fb264e0-7e82-41a0-9429-6af6fe113b5f"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.backend import clear_session\n",
        "\n",
        "# Clear previous Keras session\n",
        "clear_session()\n",
        "\n",
        "# Define the VGG-like model\n",
        "model = Sequential()\n",
        "\n",
        "# First block: 2 Conv layers with 64 filters\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second block: 2 Conv layers with 128 filters\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third block: 2 Conv layers with 256 filters\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer with 128 units\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Output layer (for example, for 10 classes)\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4elnDWnjEbmO"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "_PqNeVJZWevg",
        "outputId": "62cff7eb-a273-4aef-e8b8-a7b565469054"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get true classes\n",
        "true_classes = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                         'Dog', 'Frog', 'Horse', 'Ship', 'Truck'],\n",
        "            yticklabels=['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                         'Dog', 'Frog', 'Horse', 'Ship', 'Truck'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngY8bZcDX4ab"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in network? Why is it needed?\n",
        "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Write the answers below :**\n",
        "\n",
        "1 - Use of activation functions:\n",
        "Non-Linearity: Activation functions enable the network to learn complex patterns and relationships in the data, moving beyond linear transformations.\n",
        "\n",
        "Output Control: They constrain the output of neurons to specific ranges, which can be useful for interpreting results, especially in classification tasks.\n",
        "\n",
        "Gradient Propagation: They influence how gradients are propagated during backpropagation, which is essential for model training.\n",
        "\n",
        "Sparsity: Some activation functions promote sparsity, allowing only a subset of neurons to be activated, which can enhance model efficiency and performance.\n",
        "\n",
        "_\n",
        "\n",
        "2 - Key Differences between sigmoid and softmax:\n",
        "\n",
        "\n",
        "Sigmoid Activation Function\n",
        "Definition: The sigmoid function maps inputs to the range (0, 1).\n",
        "Output: Produces a single output value, typically used for binary classification.\n",
        "Use Case: Suitable for binary classification tasks where the goal is to predict a probability for one class versus another.\n",
        "Behavior: Can suffer from issues like the vanishing gradient problem due to its saturation in extreme input values.\n",
        "\n",
        "\n",
        "Softmax Activation Function\n",
        "Definition: The softmax function converts a vector of raw scores (logits) into probabilities.\n",
        "Output: Produces a probability distribution over multiple classes, summing to 1.\n",
        "Use Case: Used in the output layer of multi-class classification models to classify inputs into one of several classes.\n",
        "Behavior: Highlights the largest logits, providing distinct probabilities for each class based on the relative differences between the inputs.\n",
        "\n",
        "\n",
        "_\n",
        "\n",
        "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
        "\n",
        "1. Use Case\n",
        "\n",
        "Binary Crossentropy:\n",
        "Used for binary classification problems where the output is either class 0 or class 1.\n",
        "Suitable when you have only two classes.\n",
        "\n",
        "Categorical Crossentropy:\n",
        "Used for multi-class classification problems where each instance belongs to one class out of many (more than two).\n",
        "Suitable when you have multiple classes (e.g., classifying into 10 different categories).\n",
        "\n",
        "2. Output Format\n",
        "\n",
        "Binary Crossentropy:\n",
        "Expects a single output value (probability) between 0 and 1.\n",
        "The target labels are typically a single binary label (0 or 1).\n",
        "\n",
        "Categorical Crossentropy:\n",
        "Expects an output vector representing the probabilities for each class, summing to 1.\n",
        "The target labels should be one-hot encoded vectors (e.g., for 3 classes, an instance belonging to class 1 would be represented as [1, 0, 0]).\n",
        "\n",
        "3. Loss Calculation\n",
        "\n",
        "Binary Crossentropy:\n",
        "\n",
        "The formula for a single instance is:\n",
        "\n",
        "Binary¬†Crossentropy=‚àí(ylog(p)+(1‚àíy)log(1‚àíp))\n",
        "\n",
        "Here, ùë¶ is the true label (0 or 1) and ùëù is the predicted probability of the positive class.\n",
        "\n",
        "Categorical Crossentropy:\n",
        "The formula for multiple classes is:\n",
        "Categorical¬†Crossentropy=‚àí i=1‚àëC‚Äã yi ‚Äãlog(pi‚Äã )\n",
        "\n",
        "Here,C is the number of classes, ùë¶ùëñ is the true label (1 for the correct class and 0 otherwise), and ùëùùëñis the predicted probability for class i.\n",
        "\n",
        "4. Examples\n",
        "\n",
        "Binary Crossentropy:\n",
        "\n",
        "Example: Classifying emails as spam (1) or not spam (0).\n",
        "\n",
        "Categorical Crossentropy:\n",
        "\n",
        "Example: Classifying images into one of 10 classes (e.g., dogs, cats, birds, etc.).\n",
        "_\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
